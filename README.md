# ğŸš€ GPU Scheduler  
## Load-Aware GPU Task Control for Single-Machine Systems

> GPUs are powerful. They are not polite.  
> GPU Scheduler makes them behave.

---

## ğŸ“– The Story

Youâ€™re training a deep learning model.

Your GPU is already at 92%.

You quickly launch a small inference script.  
Or a visualization tool.  
Or another experiment.

Suddenly:

â€¢ Training slows down  
â€¢ Memory allocation fails  
â€¢ CUDA errors appear  
â€¢ System responsiveness drops  

The GPU didnâ€™t fail because itâ€™s weak.  
It failed because it was overloaded.

Unlike CPUs, GPUs are designed for throughput â€” not multitasking.  
They do not gracefully handle overload in single-machine environments.

And on most personal systems, nothing regulates *when* GPU tasks start.

Thatâ€™s the real problem.

---

## ğŸ¯ The Core Problem

When GPU utilization approaches full capacity:

â€¢ Memory allocation becomes fragile  
â€¢ Kernel launch latency increases  
â€¢ Throughput becomes unpredictable  
â€¢ Even lightweight tasks may fail  

Operating systems handle CPU scheduling well.  
But GPU execution start timing is largely unmanaged.

Cluster schedulers like Slurm solve this at scale.  
They are heavyweight and built for multi-node clusters.

There is a gap for lightweight scheduling on standalone machines.

---

## ğŸ’¡ The Idea

GPU Scheduler asks one simple question before launching any task:

> â€œIs it safe to start this right now?â€

If yes â†’ allow execution.  
If no â†’ wait.

There is no kernel interruption.  
No GPU driver modification.  
No forced preemption.

Just intelligent admission control.

---

## ğŸ— System Architecture

<p align="center">
  <img src="assets/architecture.png" width="850"/>
</p>



Core components:

User / Application  
Priority Task Queue  
Scheduler Engine  
Monitoring Daemon  
Resource Estimator  
GPU Hardware  
Execution Logs  

The daemon continuously monitors GPU utilization.  
The scheduler makes admission decisions.  
The queue manages waiting tasks by priority.

The GPU itself remains untouched.

---

## ğŸ”„ How It Works

1. A background daemon continuously tracks GPU utilization  
2. It detects both scheduled and externally launched GPU tasks  
3. The system estimates how much GPU a new task will require  
4. Effective load is calculated with safety headroom  
5. If total load stays below threshold (85â€“90%), the task runs  
6. Otherwise, the task waits  
7. When GPU load drops, waiting tasks are admitted  

This is preventive scheduling â€” not reactive fixing.

---

## âš™ Scheduling Strategy

Priority-Based Scheduling  
Load-Aware Admission Control  
Non-Preemptive Execution  

Tasks are ordered by importance.

Priority influences queue order, but never overrides safety thresholds.

Once a task starts, it runs without interruption.

---

## ğŸ“ˆ Why Cap Utilization at 85â€“90%?

Running GPUs at absolute 100% sounds efficient.  
In practice, it can be fragile.

Sustained 100% utilization increases:

â€¢ Power draw  
â€¢ Temperature  
â€¢ Memory contention  
â€¢ Clock fluctuation under thermal or power constraints  

Maintaining utilization headroom:

â€¢ Reduces sustained thermal stress  
â€¢ Minimizes clock instability under long workloads  
â€¢ Absorbs short load spikes  
â€¢ Improves performance consistency  
â€¢ Prevents unpredictable slowdowns  

The goal is not limiting performance.  
The goal is maintaining stability under pressure.

---

## ğŸ–¥ CPU vs GPU Scheduling Reality

CPUs:
â€¢ Lightweight preemption  
â€¢ Small execution state  
â€¢ Frequent context switching  

GPUs:
â€¢ Thousands of parallel threads  
â€¢ Large execution state  
â€¢ Expensive preemption  
â€¢ Throughput-optimized design  

Stopping a CPU task is easy.  
Stopping a GPU task mid-kernel is complex and costly.

So instead of interrupting GPU work,  
GPU Scheduler controls when tasks begin.

---

## ğŸ”¬ Optional: Cooperative Training Support

For long-running ML workloads:

Training can run inside a wrapper that supports checkpointing.

When necessary:

â€¢ The scheduler requests a safe pause  
â€¢ The model saves progress  
â€¢ A lightweight task executes  
â€¢ Training resumes from the last checkpoint  

This is cooperative, application-level scheduling â€”  
not GPU-level preemption.

---

## ğŸ” Real-World Use Cases

â€¢ Deep learning experimentation on personal workstations  
â€¢ Running inference safely alongside training  
â€¢ Shared single-GPU research environments  
â€¢ Preventing crashes from accidental concurrent launches  
â€¢ Managing GPU workloads without cluster infrastructure  

---

## ğŸ† Why This Matters

Cluster schedulers solve GPU scheduling at data center scale.

GPU Scheduler brings structured workload control to:

â€¢ Developer laptops  
â€¢ Research workstations  
â€¢ Small labs  
â€¢ Personal ML setups  

It applies operating systems principles to GPU resource management in a lightweight, practical way.

---

## ğŸ›  Future Directions

Adaptive utilization tuning  
Predictive resource modeling  
Monitoring dashboard  
Multi-user fairness policies  
Container-aware scheduling  
Integration with ML experiment pipelines  

---

## ğŸ“š Concepts Demonstrated

Operating Systems Scheduling  
Admission Control Algorithms  
Daemon-Based Monitoring  
GPU Resource Management  
Performance Stability Engineering  
Systems Architecture Design  

---
